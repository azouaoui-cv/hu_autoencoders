{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation of \n",
    "## *ENDNET: SPARSE AUTOENCODER NETWORK FOR ENDMEMBER EXTRACTION AND HYPERSPECTRAL UNMIXING*\n",
    "S. Ozkan, B. Kaya and G. B. Akar, \"EndNet: Sparse AutoEncoder Network for Endmember Extraction and Hyperspectral Unmixing,\" in IEEE Transactions on Geoscience and Remote Sensing, vol. 57, no. 1, pp. 482-496, Jan. 2019, doi: 10.1109/TGRS.2018.2856929."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss is given by \n",
    "$$\\mathcal{L} = \\frac{\\lambda_0}{2}\\|\\mathbf{x}-\\hat{\\mathbf{x}}\\|_2^2-\\lambda_1 D_\\text{KL}(1.0||C(\\mathbf{x},\\hat{\\mathbf{x}}))+\\lambda_2\\|\\mathbf{z}\\|_1+\\lambda_3\\|\\mathbf{W^{(e)}}\\|_2+\\lambda_4\\|\\mathbf{W}^{(d)}\\|_2+\\lambda_5\\|\\mathbf{\\rho}\\|_2$$\n",
    "where\n",
    "$$C(x^{(i)},x^{(j)})=1.0-\\frac{SAD(x^{(i)},x^{(j)})}{\\pi}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "from tensorflow.keras import initializers, constraints, layers, activations, regularizers\n",
    "from tensorflow.python.ops import math_ops\n",
    "from tensorflow.python.keras import backend as K\n",
    "from tensorflow.python.framework import tensor_shape\n",
    "from unmixing import HSI, plotEndmembers,vca\n",
    "from unmixing import plotEndmembersAndGT, plotAbundancesSimple, load_HSI, PlotWhileTraining\n",
    "from scipy import io as sio\n",
    "import os\n",
    "import numpy as np\n",
    "from numpy.linalg import inv\n",
    "import warnings\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method SAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SAD(y_true, y_pred):\n",
    "    A = -tf.keras.losses.cosine_similarity(y_true,y_pred)\n",
    "    sad = tf.math.acos(A)\n",
    "    return sad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method C\n",
    "method that implements \n",
    "$$C(x^{(i)},x^{(j)})=1.0-\\frac{SAD(x^{(i)},x^{(j)})}{\\pi}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def C(x,y):\n",
    "    val=1.0-SAD(x,y)/np.pi\n",
    "    return val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Cdot(object):\n",
    "    def __init__(self,vec):\n",
    "        self.vec=vec\n",
    "        \n",
    "    def dot(self, x):\n",
    "        val=1.0-SAD(self.vec,x)/np.pi\n",
    "        return val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fidelity terms\n",
    "The fidelity terms of the loss\n",
    "$$\\mathcal{L} = \\frac{\\lambda_0}{2}\\|\\mathbf{x}-\\hat{\\mathbf{x}}\\|_2^2-\\lambda_1 D_\\text{KL}(1.0||C(\\mathbf{x},\\hat{\\mathbf{x}}))$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Endnet_loss(object):\n",
    "    def __init__(self, batch_size,lambda0,lambda1):\n",
    "        self.lambda0 = lambda0\n",
    "        self.lambda1 = lambda1\n",
    "        self.b = batch_size\n",
    "    def loss(self, y_true, y_pred):\n",
    "        MSE = tf.keras.losses.mse(y_true,y_pred) \n",
    "        c=C(y_true,y_pred)\n",
    "        KL_Divergence = -tf.math.log(c)\n",
    "        loss = self.lambda0/2.0*MSE+self.lambda1*KL_Divergence\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class SumToOne\n",
    "Custom layer that enforces the ASC. Also performs regularizations l1\n",
    "$$\\lambda_2\\|\\mathbf{z}\\|_1$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SumToOne(layers.Layer):\n",
    "    def __init__(self, params, **kwargs):\n",
    "        super(SumToOne, self).__init__(**kwargs)\n",
    "        self.params = params\n",
    "    \n",
    "    def mask_all_but_top_k(self,X, k):\n",
    "        n = X.shape[1]\n",
    "        top_k_indices = tf.math.top_k(X, k).indices\n",
    "        mask = tf.reduce_sum(tf.one_hot(top_k_indices, n), axis=1)\n",
    "        return mask * X\n",
    "    \n",
    "    def l1_regularization(self,x):\n",
    "        l1 = regularizers.l1(1.0)(x)\n",
    "        return self.params['lambda2'] * l1\n",
    "        \n",
    "    def call(self, x):\n",
    "        self.add_loss(self.l1_regularization(x))\n",
    "        x = self.mask_all_but_top_k(x,2)\n",
    "        x = tf.abs(x)/(tf.reduce_sum(x, axis=-1, keepdims=True)+K.epsilon())\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class MaskedNoise\n",
    "Adds noise to the layer masked\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskedNoise(layers.Layer):\n",
    "    def __init__(self, params, **kwargs):\n",
    "        super(MaskedNoise, self).__init__(**kwargs)\n",
    "        self.std = params['noise']\n",
    "    \n",
    "        \n",
    "    def call(self, x, training=None):\n",
    "        if training:\n",
    "            mask = tf.nn.dropout(tf.ones_like(x),0.4)\n",
    "            noise = layers.GaussianNoise(self.std)(tf.zeros_like(x))\n",
    "            return x+mask*noise\n",
    "        else:\n",
    "            return x\n",
    "        \n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class Spectral_BN\n",
    "Performs batch normalization with no scaling\n",
    "$$BN(\\bf{h})=\\frac{\\bf{h}-\\bf{\\mu}}{\\sqrt{\\bf{\\sigma^2}+\\epsilon}}+\\bf{\\rho}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Spectral_BN(layers.Layer):\n",
    "    def __init__(self, params, **kwargs):\n",
    "        super(Spectral_BN, self).__init__(**kwargs)\n",
    "        self.num_outputs = params['num_endmembers']\n",
    "        self.params = params\n",
    "    \n",
    "    def l2_regularization(self,x):\n",
    "        l2 = tf.reduce_sum(tf.square(x))\n",
    "        return self.params['lambda5'] * l2\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) >= 2\n",
    "        input_dim = input_shape[-1]\n",
    "        self.p = self.add_weight(shape=(input_dim,),\n",
    "                                 initializer=\"zeros\",\n",
    "                                 trainable=True)\n",
    "    def call(self, x, training=None):\n",
    "        if training is not None:\n",
    "            mu = tf.reduce_mean(x,axis=0)\n",
    "            sigma = tf.sqrt(tf.math.reduce_variance(x,axis=0)+K.epsilon())\n",
    "            y = (x-mu)/sigma+self.p\n",
    "            self.add_loss(self.l2_regularization(self.p))\n",
    "            return y\n",
    "        else:\n",
    "            return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SparseReLU(tf.keras.layers.Layer):\n",
    "    def __init__(self):\n",
    "        super(SparseReLU, self).__init__()\n",
    "    def build(self, input_shape):\n",
    "        self.alpha = self.add_weight(shape=input_shape[1:],initializer=tf.keras.initializers.Zeros(),\n",
    "        trainable=True, constraint=tf.keras.constraints.non_neg())\n",
    "        super(SparseReLU, self).build(input_shape)\n",
    "    def call(self, x):\n",
    "        return tf.keras.backend.relu(x - self.alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class Custom_layer_transform\n",
    "Performs matrix vector multiplication using custom innner product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Custom_layer_transform(object):\n",
    "    def __init__(self,W:tf.Tensor):\n",
    "        self.W = tf.transpose(W)\n",
    "    def custom_matvec_prod(self, a:tf.Tensor):\n",
    "        cdot = Cdot(a)\n",
    "        return tf.map_fn(cdot.dot,self.W)\n",
    "    \n",
    "    def transform(self,Batch:tf.Tensor):\n",
    "        return tf.map_fn(self.custom_matvec_prod, Batch)\n",
    "        #return tf.transpose(tf.map_fn(self.custom_matvec_prod, Batch),(0,2,1))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class SAD_Layer\n",
    "This is a dense layer that transforms its inputs using a custom matrix vector product that uses normalized SAD as the inner product."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SAD_Layer(tf.keras.layers.Layer):\n",
    "    def __init__(\n",
    "        self,\n",
    "        units,\n",
    "        activation=None,\n",
    "        use_bias=False,\n",
    "        kernel_initializer=\"glorot_uniform\",\n",
    "        bias_initializer=\"zeros\",\n",
    "        kernel_regularizer=None,\n",
    "        bias_regularizer=None,\n",
    "        activity_regularizer=None,\n",
    "        kernel_constraint=None,\n",
    "        bias_constraint=None,\n",
    "        **kwargs\n",
    "    ):\n",
    "        if \"input_shape\" not in kwargs and \"input_dim\" in kwargs:\n",
    "            kwargs[\"input_shape\"] = (kwargs.pop(\"input_dim\"),)\n",
    "        super().__init__(**kwargs)\n",
    "        self.units = units\n",
    "        self.activation = tf.keras.activations.get(activation)\n",
    "        self.use_bias = use_bias\n",
    "        self.kernel_initializer = tf.keras.initializers.get(kernel_initializer)\n",
    "        self.bias_initializer = tf.keras.initializers.get(bias_initializer)\n",
    "        self.kernel_regularizer = tf.keras.regularizers.get(kernel_regularizer)\n",
    "        self.bias_regularizer = tf.keras.regularizers.get(bias_regularizer)\n",
    "        self.activity_regularizer = tf.keras.regularizers.get(activity_regularizer)\n",
    "        self.kernel_constraint = tf.keras.constraints.get(kernel_constraint)\n",
    "        self.bias_constraint = tf.keras.constraints.get(bias_constraint)\n",
    "        self.input_spec = tf.keras.layers.InputSpec(min_ndim=2)\n",
    "        self.supports_masking = True\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) >= 2\n",
    "        input_dim = input_shape[-1]\n",
    "\n",
    "        self.kernel = self.add_weight(\n",
    "            shape=(input_dim, self.units),\n",
    "            initializer=self.kernel_initializer,\n",
    "            name=\"kernel\",\n",
    "            regularizer=self.kernel_regularizer,\n",
    "            constraint=self.kernel_constraint,\n",
    "            )\n",
    "        if self.use_bias:\n",
    "                self.bias = self.add_weight(\n",
    "                    shape=(self.units,),\n",
    "                    initializer=self.bias_initializer,\n",
    "                    name=\"bias\",\n",
    "                    regularizer=self.bias_regularizer,\n",
    "                    constraint=self.bias_constraint,\n",
    "                    )\n",
    "        else:\n",
    "            self.bias = None\n",
    "        self.input_spec = tf.keras.layers.InputSpec(min_ndim=2, axes={-1: input_dim})\n",
    "        self.built = True\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        assert input_shape and len(input_shape) >= 2\n",
    "        output_shape = list(input_shape)\n",
    "        output_shape[-1] = self.units\n",
    "        return tuple(output_shape)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        print(self.kernel.get_shape())\n",
    "        print(inputs.get_shape())\n",
    "        custom_transform = Custom_layer_transform(self.kernel)\n",
    "        output = custom_transform.transform(inputs)\n",
    "        if self.use_bias:\n",
    "            output = K.bias_add(output, self.bias, data_format=\"channels_last\")\n",
    "        if self.activation is not None:\n",
    "            output = self.activation(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class Autoencoder\n",
    "Wrapper class for the autoencoder model and associcated utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder(object):\n",
    "    def __init__(self, params,W=None):\n",
    "        self.data = params[\"data\"].array()\n",
    "        self.params = params\n",
    "        self.masked_noise = MaskedNoise(params)\n",
    "        self.decoder = layers.Dense(\n",
    "            units=self.params[\"n_bands\"],\n",
    "            kernel_regularizer=regularizers.l2(self.params['lambda4']),\n",
    "            activation='linear',\n",
    "            name=\"output\",\n",
    "            kernel_constraint=constraints.non_neg(),\n",
    "            use_bias=False)\n",
    "            \n",
    "        self.hidden = SAD_Layer(\n",
    "            units=self.params[\"num_endmembers\"],\n",
    "            activation='linear',\n",
    "            kernel_regularizer=regularizers.l2(self.params['lambda3']),\n",
    "            name='hidden',\n",
    "            use_bias=False\n",
    "        )\n",
    "        self.spectral_bn = Spectral_BN(params)# BatchNormalization(scale=False)\n",
    "        self.sparse_relu = SparseReLU()\n",
    "        self.asc_layer = SumToOne(self.params, name='abundances')\n",
    "        self.model = self.create_model()\n",
    "        self.initalize_encoder_and_decoder(W)\n",
    "        self.model.compile(optimizer=self.params[\"optimizer\"], loss=self.params[\"loss\"])\n",
    "    \n",
    "    def initalize_encoder_and_decoder(self,W):\n",
    "        if W is None: return\n",
    "        self.model.get_layer('output').set_weights([W.T])\n",
    "        self.model.get_layer('hidden').set_weights([W])\n",
    "        \n",
    "        \n",
    "    def create_model(self):\n",
    "        input_features = layers.Input(shape=(self.params[\"n_bands\"],))\n",
    "        code = self.masked_noise(input_features)\n",
    "        code = self.hidden(code)\n",
    "        code = layers.BatchNormalization(scale=False)(code)\n",
    "        #code = self.spectral_bn(code)\n",
    "        code = layers.Dropout(self.params['p'])(code)\n",
    "        code = tf.keras.activations.relu(code)\n",
    "        abunds = self.asc_layer(code)\n",
    "        output = self.decoder(abunds)\n",
    "\n",
    "        return tf.keras.Model(inputs=input_features, outputs=output)\n",
    "    \n",
    "    def fix_decoder(self):\n",
    "        for l in self.model.layers:\n",
    "            l.trainable = True\n",
    "        self.model.layers[-1].trainable = False\n",
    "        self.decoder.trainable = False\n",
    "        self.model.compile(optimizer=self.params[\"optimizer\"], loss=self.params[\"loss\"])\n",
    "\n",
    "    def fix_encoder(self):\n",
    "        for l in self.model.layers:\n",
    "            l.trainable = True\n",
    "        self.model.get_layer('hidden').trainable = False\n",
    "        self.hidden.trainable = False\n",
    "        self.model.compile(optimizer=self.params[\"optimizer\"], loss=self.params[\"loss\"])\n",
    "\n",
    "        \n",
    "    \n",
    "    def fit(self,data,n):\n",
    "        plot_callback = PlotWhileTraining(n,self.params['data'])\n",
    "        return self.model.fit(\n",
    "            x=data,\n",
    "            y=data,\n",
    "            batch_size=self.params[\"batch_size\"],\n",
    "            epochs=self.params[\"epochs\"],\n",
    "            callbacks=[plot_callback]\n",
    "        )\n",
    "    \n",
    "    def train_alternating(self,data,epochs):\n",
    "        for epoch in range(epochs):\n",
    "            self.fix_decoder()\n",
    "            self.model.fit(x=data, y=data,\n",
    "                batch_size=self.params[\"batch_size\"],\n",
    "                epochs=2)\n",
    "            self.fix_encoder()\n",
    "            self.model.fit(x=data, y=data,\n",
    "                batch_size=self.params[\"batch_size\"],\n",
    "                epochs=1)\n",
    "            if epoch % 3 == 0:\n",
    "                endmembers = self.get_endmembers()\n",
    "                abundances = self.get_abundances()\n",
    "                plotEndmembersAndGT(endmembers,self.params['data'].gt)\n",
    "                plotAbundancesSimple(abundances,'abunds')\n",
    "        \n",
    "\n",
    "    def get_endmembers(self):\n",
    "        #one_hot = tf.one_hot(tf.range(0,self.params['num_endmembers']),self.params['num_endmembers'])\n",
    "        return self.model.layers[len(self.model.layers) - 1].get_weights()[0]\n",
    "\n",
    "    def get_abundances(self):\n",
    "        intermediate_layer_model = tf.keras.Model(\n",
    "            inputs=self.model.input, outputs=self.model.get_layer(\"abundances\").output\n",
    "        )\n",
    "        abundances = intermediate_layer_model.predict(self.data)\n",
    "        abundances = np.reshape(abundances,[self.params['data'].cols,self.params['data'].rows,self.params['num_endmembers']])\n",
    "        \n",
    "        return abundances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dictonary of aliases for datasets. The first string is the key and second is value (name of matfile without .mat suffix)\n",
    "#Useful when looping over datasets\n",
    "datasetnames = {\n",
    "        \"Urban\": \"Urban4\",\n",
    "        \"Samson\": \"Samson\",\n",
    "}\n",
    "dataset = \"Urban\"\n",
    "\n",
    "hsi = load_HSI(\n",
    "    \"./Datasets/\" + datasetnames[dataset] + \".mat\"\n",
    ")\n",
    "\n",
    "# Hyperparameters\n",
    "num_endmembers = 4\n",
    "num_spectra = 4000\n",
    "batch_size = 64\n",
    "learning_rate = 0.001\n",
    "epochs = 40\n",
    "lambda0 = 0.01\n",
    "lambda1 = 50.0\n",
    "lambda2 = 0.0\n",
    "lambda3 = 1e-5\n",
    "lambda4 = 1e-5\n",
    "lambda5 = 1e-3\n",
    "p = 0.1\n",
    "noise_std = 0.3\n",
    "opt = tf.optimizers.Adam(learning_rate=learning_rate,beta_1=0.7)\n",
    "\n",
    "# hsi.gt=None\n",
    "data = hsi.array()\n",
    "# Hyperparameter dictionary\n",
    "params = {\n",
    "    \"lambda0\":lambda0,\n",
    "    \"lambda1\":lambda1,\n",
    "    \"lambda2\":lambda2,\n",
    "    \"lambda3\":lambda3,\n",
    "    \"lambda4\":lambda4,\n",
    "    \"lambda5\":lambda5,\n",
    "    \"p\":p,\n",
    "    \"num_endmembers\": num_endmembers,\n",
    "    \"batch_size\": batch_size,\n",
    "    \"num_spectra\": num_spectra,\n",
    "    \"data\": hsi,\n",
    "    \"epochs\": epochs,\n",
    "    \"n_bands\": hsi.bands,\n",
    "    \"GT\": hsi.gt,\n",
    "    \"lr\": learning_rate,\n",
    "    \"optimizer\": opt,\n",
    "    \"noise\":noise_std,\n",
    "    \"loss\":Endnet_loss(batch_size,lambda0,lambda1).loss\n",
    "}\n",
    "\n",
    "plot_every = 5 #Plot endmembers and abundance maps every x epochs. Set to 0 when running experiments. \n",
    "\n",
    "training_data = data[\n",
    "    np.random.randint(0, data.shape[0], num_spectra), :\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "init = vca(data.T,params['num_endmembers'])[0]\n",
    "autoencoder = Autoencoder(params,init)\n",
    "autoencoder.fit(training_data,plot_every)\n",
    "endmembers = autoencoder.get_endmembers()\n",
    "abundances = autoencoder.get_abundances()\n",
    "plotEndmembersAndGT(endmembers, hsi.gt)\n",
    "plotAbundancesSimple(abundances,'abund.png')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Run experiment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_runs = 25\n",
    "results_folder = '/home/burkni/Hyperspectral/Review_Paper_Results'\n",
    "method_name = 'Endnet'\n",
    "\n",
    "for ds in ['Urban']:\n",
    "    opt = tf.optimizers.Adam(learning_rate=learning_rate,beta_1=0.7)\n",
    "    results_folder = './Results'\n",
    "    hsi = load_HSI(\n",
    "        \"./Datasets/\" + datasetnames[ds] + \".mat\"\n",
    "    )\n",
    "  \n",
    "    data = hsi.array()\n",
    "    params = {\n",
    "    \"lambda0\":lambda0,\n",
    "    \"lambda1\":lambda1,\n",
    "    \"lambda2\":lambda2,\n",
    "    \"lambda3\":lambda3,\n",
    "    \"lambda4\":lambda4,\n",
    "    \"lambda5\":lambda5,\n",
    "    \"p\":p,\n",
    "    \"num_endmembers\": num_endmembers,\n",
    "    \"batch_size\": batch_size,\n",
    "    \"num_spectra\": num_spectra,\n",
    "    \"data\": hsi,\n",
    "    \"epochs\": epochs,\n",
    "    \"n_bands\": hsi.bands,\n",
    "    \"GT\": hsi.gt,\n",
    "    \"lr\": learning_rate,\n",
    "    \"optimizer\": opt,\n",
    "    \"noise\":noise_std,\n",
    "    \"loss\":Endnet_loss(batch_size,lambda0,lambda1).loss\n",
    "}\n",
    "    params['data']=hsi\n",
    "    save_folder = results_folder+'/'+method_name+'/'+ds\n",
    "    if not os.path.exists(save_folder):\n",
    "        os.makedirs(save_folder)\n",
    "    for run in range(1,num_runs+1):\n",
    "        training_data = hsi.array()[np.random.randint(0, hsi.array().shape[0], num_spectra), :]\n",
    "        save_name = datasetnames[ds]+'_run'+str(run)+'.mat'\n",
    "        save_path = save_folder+'/'+save_name\n",
    "        init = vca(data.T,params['num_endmembers'])[0]\n",
    "        autoencoder = Autoencoder(params,init)\n",
    "        autoencoder.fit(training_data,epochs)\n",
    "        endmembers = autoencoder.get_endmembers()\n",
    "        abundances = autoencoder.get_abundances()\n",
    "        plotEndmembersAndGT(endmembers, hsi.gt)\n",
    "        plotAbundancesSimple(abundances,'abund.png')\n",
    "        sio.savemat(save_path,{'M':endmembers,'A':abundances})\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
