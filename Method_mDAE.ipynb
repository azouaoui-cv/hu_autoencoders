{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation of\n",
    "# *HYPERSPECTRAL IMAGE UNMIXING USING AUTOENCODER CASCADE*\n",
    "\n",
    "Published in \n",
    "\n",
    "R. Guo, W. Wang and H. Qi, \"Hyperspectral image unmixing using autoencoder cascade,\" 2015 7th Workshop on Hyperspectral Image and Signal Processing: Evolution in Remote Sensing (WHISPERS), 2015, pp. 1-4, doi: 10.1109/WHISPERS.2015.8075378."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import initializers, constraints, layers, activations, regularizers\n",
    "from tensorflow.python.ops import math_ops\n",
    "from tensorflow.python.keras import backend as K\n",
    "from tensorflow.python.framework import tensor_shape\n",
    "from unmixing import HSI, plotEndmembers, PlotWhileTraining, vca\n",
    "from unmixing import plotEndmembersAndGT, plotAbundancesSimple, load_HSI\n",
    "from scipy import io as sio\n",
    "import os\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class SumToOne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SumToOne(tf.keras.layers.Layer):\n",
    "    def __init__(self, params, **kwargs):\n",
    "        super(SumToOne, self).__init__(**kwargs)\n",
    "        self.params = params\n",
    "    def call(self, x):\n",
    "        x = K.abs(x) / (K.sum(K.abs(x), axis=-1, keepdims=True) + K.epsilon())\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class mDA_initializer\n",
    "Initializer that provides the weights for marginalized denoising autoencoder hidden layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class mDA_initializer(tf.keras.initializers.Initializer):\n",
    "    def __init__(self, data, p):\n",
    "        self.W = self.mDA(data.T, p)\n",
    "\n",
    "    def mDA(self, X, p):\n",
    "        X = np.vstack((X, np.ones((1, X.shape[1]))))\n",
    "        d = X.shape[0]\n",
    "        q = np.vstack((np.ones((d - 1, 1)) * (1 - p), np.ones(1)))\n",
    "        S = np.matmul(X, X.T)\n",
    "        Q = S * np.matmul(q, q.T)\n",
    "        row, col = np.diag_indices_from(Q)\n",
    "        Q[row, col] = np.multiply(q.T, np.diag(S))\n",
    "        P = np.multiply(S, np.repeat(q.T, d, 0))\n",
    "        a = P[0:-1, :]\n",
    "        b = Q + 1e-5 * np.eye(d)\n",
    "        W = np.linalg.lstsq(b.T, a.T, rcond=None)[0].T\n",
    "        return W.astype(np.float32)\n",
    "\n",
    "    def __call__(self, shape, dtype=None):\n",
    "        return tf.constant(value=self.W)\n",
    "\n",
    "    def get_config(self):  # To support serialization\n",
    "        return {\"W\": self.W}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class NonNeg\n",
    "Kernel regularizer to keep weights nonegative.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NonNeg(regularizers.Regularizer):\n",
    "    def __init__(self, strength):\n",
    "        super(NonNeg,self).__init__()\n",
    "        self.strength = strength\n",
    "\n",
    "    def __call__(self, x):\n",
    "        neg = tf.abs(tf.cast(x < 0, x.dtype) * x)\n",
    "        reg = self.strength * tf.reduce_sum(neg)\n",
    "        return reg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class DenseTied\n",
    "This is a dense layer that is tied to another layer and its weights are the transpose of the tied to layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenseTied(tf.keras.layers.Layer):\n",
    "    def __init__(\n",
    "        self,\n",
    "        units,\n",
    "        activation=None,\n",
    "        use_bias=False,\n",
    "        kernel_initializer=\"glorot_uniform\",\n",
    "        bias_initializer=\"zeros\",\n",
    "        kernel_regularizer=None,\n",
    "        bias_regularizer=None,\n",
    "        activity_regularizer=None,\n",
    "        kernel_constraint=None,\n",
    "        bias_constraint=None,\n",
    "        tied_to=None,\n",
    "        **kwargs\n",
    "    ):\n",
    "        self.tied_to = tied_to\n",
    "        if \"input_shape\" not in kwargs and \"input_dim\" in kwargs:\n",
    "            kwargs[\"input_shape\"] = (kwargs.pop(\"input_dim\"),)\n",
    "        super().__init__(**kwargs)\n",
    "        self.units = units\n",
    "        self.activation = tf.keras.activations.get(activation)\n",
    "        self.use_bias = use_bias\n",
    "        self.kernel_initializer = tf.keras.initializers.get(kernel_initializer)\n",
    "        self.bias_initializer = tf.keras.initializers.get(bias_initializer)\n",
    "        self.kernel_regularizer = tf.keras.regularizers.get(kernel_regularizer)\n",
    "        self.bias_regularizer = tf.keras.regularizers.get(bias_regularizer)\n",
    "        self.activity_regularizer = tf.keras.regularizers.get(activity_regularizer)\n",
    "        self.kernel_constraint = tf.keras.constraints.get(kernel_constraint)\n",
    "        self.bias_constraint = tf.keras.constraints.get(bias_constraint)\n",
    "        self.input_spec = tf.keras.layers.InputSpec(min_ndim=2)\n",
    "        self.supports_masking = True\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) >= 2\n",
    "        input_dim = input_shape[-1]\n",
    "\n",
    "        if self.tied_to is not None:\n",
    "            self.kernel = K.transpose(self.tied_to.kernel)\n",
    "            self.non_trainable_weights.append(self.kernel)\n",
    "        else:\n",
    "            self.kernel = self.add_weight(\n",
    "                shape=(input_dim, self.units),\n",
    "                initializer=self.kernel_initializer,\n",
    "                name=\"kernel\",\n",
    "                regularizer=self.kernel_regularizer,\n",
    "                constraint=self.kernel_constraint,\n",
    "            )\n",
    "        if self.use_bias:\n",
    "            self.bias = self.add_weight(\n",
    "                shape=(self.units,),\n",
    "                initializer=self.bias_initializer,\n",
    "                name=\"bias\",\n",
    "                regularizer=self.bias_regularizer,\n",
    "                constraint=self.bias_constraint,\n",
    "            )\n",
    "        else:\n",
    "            self.bias = None\n",
    "        self.input_spec = tf.keras.layers.InputSpec(min_ndim=2, axes={-1: input_dim})\n",
    "        self.built = True\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        assert input_shape and len(input_shape) >= 2\n",
    "        output_shape = list(input_shape)\n",
    "        output_shape[-1] = self.units\n",
    "        return tuple(output_shape)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        output = K.dot(inputs, self.kernel)\n",
    "        if self.use_bias:\n",
    "            output = K.bias_add(output, self.bias, data_format=\"channels_last\")\n",
    "        if self.activation is not None:\n",
    "            output = self.activation(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class AugmentedLogistic(Layer)\n",
    "Custom layer that implements the augmented logistic activation given by\n",
    "$$\\frac{1}{1+e^{a_ix_i-b_i}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AugmentedLogistic(tf.keras.layers.Layer):\n",
    "    def __init__(self):\n",
    "        super(AugmentedLogistic, self).__init__()\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) >= 2\n",
    "        input_dim = input_shape[-1]\n",
    "        self.a = self.add_weight(shape=(input_dim,),\n",
    "                                 initializer=\"ones\",\n",
    "                                 trainable=True)\n",
    "        self.b = self.add_weight(shape=(input_dim,),\n",
    "                                 initializer=\"zeros\",\n",
    "                                 trainable=True)\n",
    "    def call(self, x):\n",
    "        y = self.a*x + self.b\n",
    "        return tf.nn.sigmoid(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class Autoencoder\n",
    "Wrapper class for the autoencoder model and associcated utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder(object):\n",
    "    def __init__(self, params):\n",
    "        self.data = params[\"data\"].array()\n",
    "        self.params = params\n",
    "        self.mDA_layer = layers.Dense(\n",
    "            units=self.params[\"n_bands\"] + 1,\n",
    "            activation=\"linear\",\n",
    "            use_bias=False,\n",
    "            kernel_initializer=mDA_initializer(self.data, self.params[\"p\"]),\n",
    "        )\n",
    "        self.unmix_layer = layers.Dense(\n",
    "            units=self.params[\"num_endmembers\"],\n",
    "            activation=\"linear\",\n",
    "            kernel_regularizer=NonNeg(10),\n",
    "            name='unmix',\n",
    "            use_bias=False\n",
    "        )\n",
    "        self.output_layer = DenseTied(\n",
    "            units=self.params[\"n_bands\"],\n",
    "            kernel_constraint=None,\n",
    "            activation=\"linear\",\n",
    "            tied_to=self.unmix_layer,\n",
    "        )\n",
    "        self.asc_layer = SumToOne(self.params, name = 'abundances')\n",
    "        self.model = self.create_model()\n",
    "        init = vca(data.T,params['num_endmembers'])[0]\n",
    "        self.model.get_layer(name='unmix').set_weights([init])\n",
    "        self.model.compile(optimizer=self.params[\"optimizer\"], loss=self.params[\"loss\"])\n",
    "\n",
    "    def create_model(self):\n",
    "        \n",
    "        input_features = layers.Input(shape=(self.params[\"n_bands\"],))\n",
    "        code = self.mDA_layer(input_features)\n",
    "        code = layers.Lambda(lambda x: x[:, :-1])(code)\n",
    "        code = self.unmix_layer(code)\n",
    "        code = AugmentedLogistic()(code)\n",
    "        \n",
    "        abunds = self.asc_layer(code)\n",
    "        output = self.output_layer(abunds)\n",
    "\n",
    "        return tf.keras.Model(inputs=input_features, outputs=output)\n",
    "\n",
    "    def fit(self,data,n):\n",
    "        plot_callback = PlotWhileTraining(n,self.params['data'])\n",
    "        return self.model.fit(\n",
    "            x=data,\n",
    "            y=data,\n",
    "            batch_size=self.params[\"batch_size\"],\n",
    "            epochs=self.params[\"epochs\"],\n",
    "            callbacks=[plot_callback]\n",
    "        )\n",
    "    \n",
    "    def train_alternating(self,data,epochs):\n",
    "        for epoch in range(epochs):\n",
    "            self.fix_decoder()\n",
    "            self.model.fit(x=data, y=data,\n",
    "                batch_size=self.params[\"batch_size\"],\n",
    "                epochs=2)\n",
    "            self.fix_encoder()\n",
    "            self.model.fit(x=data, y=data,\n",
    "                batch_size=self.params[\"batch_size\"],\n",
    "                epochs=1)\n",
    "\n",
    "\n",
    "    def get_endmembers(self):\n",
    "        return self.model.layers[len(self.model.layers) - 1].get_weights()[0]\n",
    "\n",
    "    def get_abundances(self):\n",
    "        intermediate_layer_model = tf.keras.Model(\n",
    "            inputs=self.model.input, outputs=self.model.get_layer(\"abundances\").output\n",
    "        )\n",
    "        abundances = intermediate_layer_model.predict(self.data)\n",
    "        abundances = np.reshape(abundances,[self.params['data'].cols,self.params['data'].rows,self.params['num_endmembers']])\n",
    "        \n",
    "        return abundances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Hyperparameters and load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dictonary of aliases for datasets. The first string is the key and second is value (name of matfile without .mat suffix)\n",
    "#Useful when looping over datasets\n",
    "datasetnames = {\n",
    "        \"Urban\": \"Urban4\",\n",
    "}\n",
    "dataset = \"Urban\"\n",
    "\n",
    "hsi = load_HSI(\n",
    "    \"./Datasets/\" + datasetnames[dataset] + \".mat\"\n",
    ")\n",
    "data = hsi.array()\n",
    "\n",
    "\n",
    "# Hyperparameters\n",
    "num_endmembers = 4\n",
    "num_spectra = 90000\n",
    "batch_size = 64\n",
    "learning_rate = 0.001\n",
    "epochs = 10\n",
    "loss = \"mse\"\n",
    "opt = tf.optimizers.RMSprop(learning_rate=learning_rate,momentum=0.9)\n",
    "\n",
    "\n",
    "# Hyperparameter dictionary\n",
    "params = {\n",
    "    \"num_endmembers\": num_endmembers,\n",
    "    \"batch_size\": batch_size,\n",
    "    \"num_spectra\": num_spectra,\n",
    "    \"data\": hsi,\n",
    "    \"epochs\": epochs,\n",
    "    \"n_bands\": hsi.bands,\n",
    "    \"GT\": hsi.gt,\n",
    "    \"lr\": learning_rate,\n",
    "    \"p\": 0.01,\n",
    "    \"optimizer\": opt,\n",
    "    \"loss\": loss,\n",
    "}\n",
    "\n",
    "plot_every = 0 #Plot endmembers and abundance maps every x epochs. Set to 0 when running experiments. \n",
    "\n",
    "training_data = data[np.random.randint(0, data.shape[0], num_spectra), :]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder = Autoencoder(params)\n",
    "autoencoder.fit(training_data,plot_every)\n",
    "endmembers = autoencoder.get_endmembers()\n",
    "print(endmembers.shape)\n",
    "plotEndmembersAndGT(endmembers, hsi.gt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run experiment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_runs = 25\n",
    "results_folder = './Results'\n",
    "method_name = 'mDAE'\n",
    "\n",
    "for dataset in ['Urban']:\n",
    "    save_folder = results_folder+'/'+method_name+'/'+dataset\n",
    "    if not os.path.exists(save_folder):\n",
    "        os.makedirs(save_folder)\n",
    "\n",
    "    hsi = load_HSI(\n",
    "        \"./Datasets/\" + datasetnames[dataset] + \".mat\"\n",
    "    )\n",
    "    data=hsi.array()\n",
    "    for run in range(1,num_runs+1):\n",
    "        training_data = data[np.random.randint(0, data.shape[0], num_spectra), :]\n",
    "        save_name = dataset+'_run'+str(run)+'.mat'\n",
    "        save_path = save_folder+'/'+save_name\n",
    "        autoencoder = Autoencoder(params)\n",
    "        autoencoder.fit(training_data,plot_every)\n",
    "        endmembers = autoencoder.get_endmembers()\n",
    "        abundances = autoencoder.get_abundances()\n",
    "        plotEndmembersAndGT(endmembers, hsi.gt)\n",
    "        plotAbundancesSimple(abundances,'abund.png')\n",
    "        sio.savemat(save_path,{'M':endmembers,'A':abundances})\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
