{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation of \n",
    "# *Hyperspectral Unmixing Using Orthogonal Sparse Prior-Based Autoencoder With Hyper-Laplacian Loss and Data-Driven Outlier Detection*\n",
    "Z. Dou, K. Gao, X. Zhang, H. Wang and J. Wang, \"Hyperspectral Unmixing Using Orthogonal Sparse Prior-Based Autoencoder With Hyper-Laplacian Loss and Data-Driven Outlier Detection,\" in IEEE Transactions on Geoscience and Remote Sensing, vol. 58, no. 9, pp. 6550-6564, Sept. 2020, doi: 10.1109/TGRS.2020.2977819."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss is given by \n",
    "$$E = \\|\\bf{X}-\\bf{R}\\|_p^p-\\sum_i\\log\\left(\\frac{\\bf{R}_i\\cdot\\bf{X}_i}{\\|(\\bf{W}\\bf{H})_i\\|_2\\|\\bf{X}_i\\|_2}\\right) + \\rho_1 L_\\text{OSP}+\\rho_2\\sum_i L_1(\\bf{h_i})-\\rho_3 S(M_{W_{<0}}\\odot W)+\\rho_4 S(M_{W_{>0}}\\odot W)$$\n",
    "where\n",
    "$$L_\\text{OSP}(B)=\\sum_{i<j}\\frac{\\bf{B_i}\\cdot\\bf{B_j}}{\\|\\bf{B}_i\\|_2\\|\\bf{B}_j\\|_2}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import initializers, constraints, layers, activations, regularizers\n",
    "from tensorflow.python.ops import math_ops\n",
    "from tensorflow.python.keras import backend as K\n",
    "from tensorflow.python.framework import tensor_shape\n",
    "from unmixing import HSI, plotEndmembers,SAD, vca\n",
    "from unmixing import plotEndmembersAndGT, plotAbundancesSimple, load_HSI, PlotWhileTraining\n",
    "from scipy import io as sio\n",
    "import os\n",
    "import numpy as np\n",
    "from numpy.linalg import inv\n",
    "import warnings\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method OSP\n",
    "method that implements \n",
    "$$L_\\text{OSP}(B)=\\sum_{i<j}\\frac{\\bf{B_i}\\cdot\\bf{B_j}}{\\|\\bf{B}_i\\|_2\\|\\bf{B}_j\\|_2}$$\n",
    "This makes abundance maps mutually orthogonal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def OSP(B,R):\n",
    "    dots = 0.0\n",
    "    B = tf.linalg.l2_normalize(B,axis=0)\n",
    "    for i in range(R):\n",
    "        for j in range(i+1,R):\n",
    "            A1 = B[:,i]\n",
    "            A2 = B[:,j]\n",
    "            dot = tf.reduce_sum(A1*A2,axis=0)\n",
    "            dots = dots + dot\n",
    "    return dots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class SumToOne\n",
    "Also performs regularizations l1 and OSP \n",
    "$$L_\\text{OSP}+\\rho_2\\sum_i L_1(\\bf{h_i})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SumToOne(layers.Layer):\n",
    "    def __init__(self, params, **kwargs):\n",
    "        super(SumToOne, self).__init__(**kwargs)\n",
    "        self.num_outputs = params['num_endmembers']\n",
    "        self.params = params\n",
    "        \n",
    "    def l1_regularization(self,x):\n",
    "        l1 = tf.reduce_sum(tf.pow(tf.abs(x)+1e-8,0.7))\n",
    "        return self.params['l1'] * l1\n",
    "    \n",
    "    def osp_regularization(self,x):\n",
    "        return self.params['osp']*OSP(x,self.params['num_endmembers'])\n",
    "        \n",
    "    def call(self, x):\n",
    "        x = tf.nn.softmax(self.params['scale'] * x)\n",
    "        self.add_loss(self.l1_regularization(x))\n",
    "        self.add_loss(self.osp_regularization(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class NonNegLessOne\n",
    "Kernel regularizer to keep weights in range 0 to 1. Implements \n",
    "$$-\\rho_3 S(M_{W_{<0}}\\odot W)+\\rho_4 S(M_{W_{>0}}\\odot W)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NonNegLessOne(regularizers.Regularizer):\n",
    "    def __init__(self, strength):\n",
    "        super(NonNegLessOne,self).__init__()\n",
    "        self.strength = strength\n",
    "\n",
    "    def __call__(self, x):\n",
    "        neg = tf.cast(x < 0, x.dtype) * x\n",
    "        greater_one = tf.cast(x>=1.0, x.dtype)*x\n",
    "        reg = -self.strength * tf.reduce_sum(neg)+self.strength*tf.reduce_sum(greater_one)\n",
    "        return reg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class HyperLaplacianLoss\n",
    "Calculates the Hyper Laplacian loss\n",
    "$$\\|\\bf{X}-\\bf{R}\\|_p^p-\\sum_i\\log\\left(\\frac{\\bf{R}_i\\cdot\\bf{X}_i}{\\|(\\bf{W}\\bf{H})_i\\|_2\\|\\bf{X}_i\\|_2}\\right)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HyperLaplacianLoss(object):\n",
    "    def __init__(self,scale):\n",
    "        super(HyperLaplacianLoss).__init__()\n",
    "        self.scale = scale\n",
    "        \n",
    "    def loss(self,X,R):\n",
    "        fidelity = tf.reduce_mean(tf.pow(tf.abs(X-R)+tf.keras.backend.epsilon(),0.7),axis=None)\n",
    "        x = tf.linalg.l2_normalize(X,axis=1)\n",
    "        r = tf.linalg.l2_normalize(R,axis=1)\n",
    "        s = X.get_shape().as_list()\n",
    "        log_cosines = tf.reduce_sum(tf.math.log(tf.reduce_sum(r*x,axis=1)+K.epsilon()))\n",
    "        return self.scale*fidelity - log_cosines\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class Autoencoder\n",
    "Wrapper class for the autoencoder model and associcated utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder(object):\n",
    "    def __init__(self, params,W=None):\n",
    "        self.data = params[\"data\"].array()\n",
    "        self.params = params\n",
    "        self.decoder = layers.Dense(\n",
    "            units=self.params[\"n_bands\"],\n",
    "            kernel_regularizer=NonNegLessOne(10),\n",
    "            activation='linear',\n",
    "            name=\"output\",\n",
    "            use_bias=False,\n",
    "            kernel_constraint=None)\n",
    "        self.hidden1 = layers.Dense(\n",
    "            units=self.params[\"num_endmembers\"],\n",
    "            activation=self.params[\"activation\"],\n",
    "            name='hidden1',\n",
    "            use_bias=True\n",
    "        )\n",
    "        self.hidden2 = layers.Dense(\n",
    "            units=self.params[\"num_endmembers\"],\n",
    "            activation='linear',\n",
    "            name='hidden2',\n",
    "            use_bias=True\n",
    "        )\n",
    "\n",
    "        self.asc_layer = SumToOne(self.params, name='abundances')\n",
    "        self.model = self.create_model()\n",
    "        self.initalize_encoder_and_decoder(W)\n",
    "        self.model.compile(optimizer=self.params[\"optimizer\"], loss=self.params[\"loss\"])\n",
    "    \n",
    "    def initalize_encoder_and_decoder(self,W):\n",
    "        if W is None: return\n",
    "        self.model.get_layer('output').set_weights([W.T])\n",
    "        self.model.get_layer('hidden1').set_weights([W,np.zeros(self.params[\"num_endmembers\"])])\n",
    "        W2 = inv(np.matmul(W.T,W))\n",
    "        self.model.get_layer('hidden2').set_weights([W2,np.zeros(self.params[\"num_endmembers\"])])\n",
    "        \n",
    "        \n",
    "    def create_model(self):\n",
    "        input_features = layers.Input(shape=(self.params[\"n_bands\"],))\n",
    "        code = self.hidden1(input_features)\n",
    "        code = self.hidden2(code)\n",
    "        code = layers.BatchNormalization()(code)\n",
    "        abunds = self.asc_layer(code)\n",
    "        output = self.decoder(abunds)\n",
    "\n",
    "        return tf.keras.Model(inputs=input_features, outputs=output)\n",
    "    \n",
    "    def fix_decoder(self):\n",
    "        for l in self.model.layers:\n",
    "            l.trainable = True\n",
    "        self.model.layers[-1].trainable = False\n",
    "        self.decoder.trainable = False\n",
    "        self.model.compile(optimizer=self.params[\"optimizer\"], loss=self.params[\"loss\"])\n",
    "\n",
    "    def fix_encoder(self):\n",
    "        for l in self.model.layers:\n",
    "            l.trainable = True\n",
    "        self.model.get_layer('hidden1').trainable = False\n",
    "        self.model.get_layer('hidden2').trainable = False\n",
    "        self.hidden1.trainable = False\n",
    "        self.hidden2.trainable = False\n",
    "        self.model.compile(optimizer=self.params[\"optimizer\"], loss=self.params[\"loss\"])\n",
    "\n",
    "        \n",
    "    \n",
    "    def fit(self,data,n):\n",
    "        plot_callback = PlotWhileTraining(n,self.params['data'])\n",
    "        return self.model.fit(\n",
    "            x=data,\n",
    "            y=data,\n",
    "            batch_size=self.params[\"batch_size\"],\n",
    "            epochs=self.params[\"epochs\"],\n",
    "            callbacks=[plot_callback]\n",
    "        )\n",
    "    \n",
    "    def train_alternating(self,data,epochs):\n",
    "        for epoch in range(epochs):\n",
    "            self.fix_decoder()\n",
    "            self.model.fit(x=data, y=data,\n",
    "                batch_size=self.params[\"batch_size\"],\n",
    "                epochs=2)\n",
    "            self.fix_encoder()\n",
    "            self.model.fit(x=data, y=data,\n",
    "                batch_size=self.params[\"batch_size\"],\n",
    "                epochs=1)\n",
    "\n",
    "    def get_endmembers(self):\n",
    "        return self.model.layers[len(self.model.layers) - 1].get_weights()[0]\n",
    "\n",
    "    def get_abundances(self):\n",
    "        intermediate_layer_model = tf.keras.Model(\n",
    "            inputs=self.model.input, outputs=self.model.get_layer(\"abundances\").output\n",
    "        )\n",
    "        abundances = intermediate_layer_model.predict(self.data)\n",
    "        abundances = np.reshape(abundances,[self.params['data'].cols,self.params['data'].rows,self.params['num_endmembers']])\n",
    "        \n",
    "        return abundances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class OutlierDetection\n",
    "Implements data driven outlier detection using heat kernel smoothing and thresholding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OutlierDetection(object):\n",
    "    def __init__(self,image,alpha,threshold):\n",
    "        self.I = image\n",
    "        self.alpha = alpha\n",
    "        self.threshold = threshold\n",
    "    \n",
    "    def get_neighbors(self,row,column):\n",
    "        n,m,b = self.I.shape\n",
    "        neighbors_x = np.s_[max(row-1,0):min(row+1,n-1)+1]\n",
    "        neighbors_y = np.s_[max(column-1,0):min(column+1,m-1)+1]\n",
    "        block = np.zeros((3,3,b))\n",
    "        block_x = np.s_[max(row-1,0)-row+1:min(row+1,n-1)+1-row+1]\n",
    "        block_y = np.s_[max(column-1,0)-column+1:min(column+1,m-1)+1-column+1]\n",
    "        block[block_x,block_y] = self.I[neighbors_x,neighbors_y,:]\n",
    "        block = np.reshape(block,(9,-1))\n",
    "        block = np.delete(block,5,0)\n",
    "        return block\n",
    "    \n",
    "    def d(self,x,y):\n",
    "        return np.linalg.norm(x-y)**2 \n",
    "    \n",
    "    def s(self,row,column):\n",
    "        N = self.get_neighbors(row,column)\n",
    "        x0 = self.I[row,column,:]\n",
    "        dists = list(map(lambda x:self.d(x0,x),N))\n",
    "        return 1/8*sum(list(map(lambda x:np.exp(-x/self.alpha),dists)))\n",
    "    \n",
    "    def create_heatmap(self):\n",
    "        n,m,b = self.I.shape\n",
    "        M = np.zeros((n,m))\n",
    "        for i in range(n):\n",
    "            for j in range(m):\n",
    "                M[j,i]=self.s(i,j)\n",
    "        return M\n",
    "    \n",
    "    def get_training_data(self):\n",
    "        M = self.create_heatmap()\n",
    "        maxM = np.max(M.flatten())\n",
    "        indices = np.argwhere(M>self.threshold)\n",
    "        arr = np.zeros((indices.shape[0],self.I.shape[2]))\n",
    "        i=0\n",
    "        for [r,c] in indices:\n",
    "            arr[i,:]=self.I[r,c,:]\n",
    "            i=i+1\n",
    "        return [arr,M]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data and detect outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dictonary of aliases for datasets. The first string is the key and second is value (name of matfile without .mat suffix)\n",
    "#Useful when looping over datasets\n",
    "datasetnames = {\n",
    "    \"Urban\": \"Urban4\",\n",
    "}\n",
    "dataset = \"Urban\"\n",
    "hsi = load_HSI(\n",
    "    \"./Datasets/\" + datasetnames[dataset] + \".mat\"\n",
    ")\n",
    "data,hmap = OutlierDetection(hsi.image,0.05,0.5).get_training_data()\n",
    "plt.figure(figsize=(12,12))\n",
    "plt.imshow(hmap,cmap='gray')\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IsOutlierDetection = True\n",
    "\n",
    "# Hyperparameters\n",
    "num_endmembers = 4\n",
    "num_spectra = 2000\n",
    "batch_size = 15\n",
    "learning_rate = 0.001\n",
    "epochs = 13\n",
    "n_bands = hsi.bands\n",
    "\n",
    "opt = tf.optimizers.Adam(learning_rate=learning_rate)\n",
    "activation = 'relu'\n",
    "l1 = 1.0\n",
    "osp = 0.5\n",
    "\n",
    "# hsi.gt=None\n",
    "\n",
    "if IsOutlierDetection:\n",
    "    data,hmap = OutlierDetection(hsi.image,0.05,0.5).get_training_data()\n",
    "    num_spectra = data.shape[0]\n",
    "    batch_size = 256\n",
    "else:\n",
    "    data = hsi.array()\n",
    "\n",
    "fid_scale = batch_size\n",
    "loss = HyperLaplacianLoss(fid_scale).loss\n",
    "    \n",
    "# Hyperparameter dictionary\n",
    "params = {\n",
    "    \"activation\": activation,\n",
    "    \"num_endmembers\": num_endmembers,\n",
    "    \"batch_size\": batch_size,\n",
    "    \"num_spectra\": num_spectra,\n",
    "    \"data\": hsi,\n",
    "    \"epochs\": epochs,\n",
    "    \"n_bands\":n_bands ,\n",
    "    \"GT\": hsi.gt,\n",
    "    \"lr\": learning_rate,\n",
    "    \"optimizer\": opt,\n",
    "    \"loss\": loss,\n",
    "    \"scale\": 1,\n",
    "    \"l1\": l1,\n",
    "    \"osp\": osp\n",
    "}\n",
    "\n",
    "training_data = data[\n",
    "    np.random.randint(0, data.shape[0], num_spectra), :\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vca_end = vca(data.T,num_endmembers)[0]\n",
    "autoencoder = Autoencoder(params,vca_end)\n",
    "autoencoder.train_alternating(training_data,epochs)\n",
    "endmembers = autoencoder.get_endmembers()\n",
    "abundances = autoencoder.get_abundances()\n",
    "plotEndmembersAndGT(endmembers, hsi.gt)\n",
    "plotAbundancesSimple(abundances,'abund.png')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run experiment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_runs = 25\n",
    "results_folder = './Results'\n",
    "method_name = 'OSPAEU'\n",
    "\n",
    "#Dictonary of aliases for datasets. The first string is the key and second is value (name of matfile without .mat suffix)\n",
    "#Useful when looping over datasets\n",
    "datasetnames = {\"Urban\":\"Urban4\"}\n",
    "\n",
    "for dataset in ['Urban']:\n",
    "    save_folder = results_folder+'/'+method_name+'/'+dataset\n",
    "    if not os.path.exists(save_folder):\n",
    "        os.makedirs(save_folder)\n",
    "    dataset_name = dataset\n",
    "\n",
    "    hsi = load_HSI(\n",
    "        \"./Datasets/\" + datasetnames[dataset] + \".mat\"\n",
    "    )\n",
    "    hsi.image = hsi.image-np.min(hsi.image,axis=2,keepdims=True)+ 0.000001 #negative values cause trouble\n",
    "    data,hmap = OutlierDetection(hsi.image,0.05,0.5).get_training_data()\n",
    "    \n",
    "    num_spectra = data.shape[0]\n",
    "    batch_size = 256\n",
    "    params['num_spectra']=num_spectra\n",
    "    params['data']=hsi\n",
    "    params['n_bands']=hsi.bands\n",
    "\n",
    "    for run in range(1,num_runs+1):\n",
    "        opt = tf.optimizers.Adam(learning_rate=learning_rate)\n",
    "        params['optimizer']=opt\n",
    "        training_data = data[np.random.randint(0, data.shape[0], num_spectra), :]\n",
    "        save_name = dataset+'_run'+str(run)+'.mat'\n",
    "        save_path = save_folder+'/'+save_name\n",
    "        vca_end = vca(data.T,num_endmembers)[0]\n",
    "        autoencoder = Autoencoder(params,vca_end)\n",
    "        autoencoder.train_alternating(training_data,epochs)\n",
    "        endmembers = autoencoder.get_endmembers()\n",
    "        abundances = autoencoder.get_abundances()\n",
    "        plotEndmembersAndGT(endmembers, hsi.gt)\n",
    "        plotAbundancesSimple(abundances,'abund.png')\n",
    "        sio.savemat(save_path,{'M':endmembers,'A':abundances})\n",
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
