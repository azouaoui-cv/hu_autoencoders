{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Unmixing Autoencoder (DAEU)\n",
    "Implementation of\n",
    "\n",
    "B. Palsson, J. Sigurdsson, J. R. Sveinsson and M. O. Ulfarsson, \"Hyperspectral Unmixing Using a Neural Network Autoencoder,\" in IEEE Access, vol. 6, pp. 25646-25656, 2018, doi: 10.1109/ACCESS.2018.2818280."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import initializers, constraints, layers, activations, regularizers\n",
    "from tensorflow.python.ops import math_ops\n",
    "from tensorflow.python.keras import backend as K\n",
    "from tensorflow.python.framework import tensor_shape\n",
    "from unmixing import HSI, plotEndmembers,SAD\n",
    "from unmixing import plotEndmembersAndGT, plotAbundancesSimple, load_HSI, PlotWhileTraining\n",
    "from sklearn.feature_extraction.image import extract_patches_2d\n",
    "from scipy import io as sio\n",
    "import os\n",
    "import numpy as np\n",
    "from numpy.linalg import inv\n",
    "import warnings\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class SumToOne\n",
    "Custom layer that enforces the ASC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SumToOne(layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(SumToOne, self).__init__(**kwargs)\n",
    "        \n",
    "    def call(self, x):\n",
    "        x *= K.cast(x >= K.epsilon(), K.floatx())\n",
    "        x = K.relu(x)\n",
    "        x = x/(K.sum(x, axis=-1, keepdims=True)+K.epsilon())\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layer with soft thresholding ReLU activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SparseReLU(tf.keras.layers.Layer):\n",
    "    def __init__(self,params):\n",
    "        self.params=params\n",
    "        super(SparseReLU, self).__init__()\n",
    "        self.alpha = self.add_weight(shape=(self.params['num_endmembers'],),initializer=tf.keras.initializers.Zeros(),\n",
    "        trainable=True, constraint=tf.keras.constraints.non_neg())\n",
    "    def build(self, input_shape):\n",
    "        self.alpha = self.add_weight(shape=input_shape[1:],initializer=tf.keras.initializers.Zeros(),\n",
    "        trainable=True, constraint=tf.keras.constraints.non_neg())\n",
    "        super(SparseReLU, self).build(input_shape)\n",
    "    def call(self, x):\n",
    "        return tf.keras.backend.relu(x - self.alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder(object):\n",
    "    def __init__(self, params):\n",
    "        self.data = None\n",
    "        self.params = params\n",
    "        self.is_deep = True\n",
    "        self.model = self.create_model()\n",
    "        self.model.compile(optimizer=self.params[\"optimizer\"], loss=self.params[\"loss\"])\n",
    "        \n",
    "        \n",
    "    def create_model(self):\n",
    "        use_bias = False\n",
    "        n_end = self.params['num_endmembers']\n",
    "        # Input layer\n",
    "        Sparse_ReLU = SparseReLU(self.params)\n",
    "        input_ = layers.Input(shape=(self.params['n_bands'],))\n",
    "          \n",
    "        if self.is_deep:\n",
    "            encoded = layers.Dense(n_end * 9, use_bias=use_bias,\n",
    "                            activation=self.params['activation'])(input_)\n",
    "            encoded = layers.Dense(n_end * 6, use_bias=use_bias,\n",
    "                            activation=self.params['activation'])(encoded)\n",
    "            #encoded = layers.BatchNormalization()(encoded)\n",
    "            encoded = layers.Dense(n_end * 3, use_bias=use_bias,activation=self.params['activation'])(encoded)\n",
    "            #encoded = layers.BatchNormalization()(encoded)\n",
    "            encoded = layers.Dense(n_end, use_bias=use_bias,\n",
    "                            activation=self.params['activation'])(encoded)\n",
    "        else:\n",
    "            encoded = Dense(n_end, use_bias=use_bias, activation=self.params['activation'], activity_regularizer=None,\n",
    "                            kernel_regularizer=None)(input_)\n",
    "        # Utility Layers\n",
    "\n",
    "        # Batch Normalization\n",
    "        encoded = layers.BatchNormalization()(encoded)\n",
    "        # Soft Thresholding\n",
    "        encoded = Sparse_ReLU(encoded)\n",
    "        # Sum To One (ASC)\n",
    "        encoded = SumToOne(name='abundances')(encoded)\n",
    "\n",
    "        # Gaussian Dropout\n",
    "        decoded = layers.GaussianDropout(0.0045)(encoded)\n",
    "\n",
    "        # Decoder\n",
    "        decoded = layers.Dense(self.params['n_bands'], activation='linear', name='endmembers',\n",
    "                        use_bias=False,\n",
    "                        kernel_constraint=constraints.non_neg())(\n",
    "            encoded)\n",
    "\n",
    "        return tf.keras.Model(inputs=input_ , outputs=decoded)\n",
    "    \n",
    "    def fit(self,data,plot_every):\n",
    "        plot_callback = PlotWhileTraining(plot_every,self.params['data'])\n",
    "        return self.model.fit(\n",
    "            x=data,\n",
    "            y=data,\n",
    "            batch_size=self.params[\"batch_size\"],\n",
    "            epochs=self.params[\"epochs\"],\n",
    "            callbacks=[plot_callback]\n",
    "        )\n",
    "        \n",
    "\n",
    "    def get_endmembers(self):\n",
    "        return self.model.layers[len(self.model.layers) - 1].get_weights()[0]\n",
    "\n",
    "    def get_abundances(self):\n",
    "        intermediate_layer_model = tf.keras.Model(\n",
    "            inputs=self.model.input, outputs=self.model.get_layer(\"abundances\").output\n",
    "        )\n",
    "        abundances = intermediate_layer_model.predict(self.params['data'].array())\n",
    "        abundances = np.reshape(abundances,[self.params['data'].cols,self.params['data'].rows,self.params['num_endmembers']])\n",
    "        \n",
    "        return abundances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Hyperparameters and load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dictonary of aliases for datasets. The first string is the key and second is value (name of matfile without .mat suffix)\n",
    "#Useful when looping over datasets\n",
    "datasetnames = {'Urban':'Urban4'} \n",
    "\n",
    "\n",
    "dataset = \"Urban\"\n",
    "\n",
    "hsi = load_HSI(\n",
    "    \"./Datasets/\" + datasetnames[dataset] + \".mat\"\n",
    ")\n",
    "\n",
    "\n",
    "# Hyperparameters\n",
    "num_endmembers = 4\n",
    "num_spectra = 2000\n",
    "batch_size = 6\n",
    "learning_rate = 0.001\n",
    "epochs = 40\n",
    "loss = SAD\n",
    "opt = tf.optimizers.RMSprop(learning_rate=learning_rate)\n",
    "\n",
    "data = hsi.array()\n",
    "\n",
    "# Hyperparameter dictionary\n",
    "params = {\n",
    "    \"num_endmembers\": num_endmembers,\n",
    "    \"batch_size\": batch_size,\n",
    "    \"num_spectra\": num_spectra,\n",
    "    \"data\": hsi,\n",
    "    \"epochs\": epochs,\n",
    "    \"n_bands\": hsi.bands,\n",
    "    \"GT\": hsi.gt,\n",
    "    \"lr\": learning_rate,\n",
    "    \"optimizer\": opt,\n",
    "    \"loss\": loss,\n",
    "    \"activation\":layers.LeakyReLU(0.1)\n",
    "}\n",
    "\n",
    "plot_every = 0 #Plot endmembers and abundance maps every x epochs. Set to 0 when running experiments. \n",
    "\n",
    "training_data = data[\n",
    "    np.random.randint(0, data.shape[0], num_spectra), :\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "autoencoder = Autoencoder(params)\n",
    "autoencoder.fit(training_data,plot_every)\n",
    "endmembers = autoencoder.get_endmembers()\n",
    "abundances = autoencoder.get_abundances()\n",
    "plotEndmembersAndGT(endmembers, hsi.gt)\n",
    "plotAbundancesSimple(abundances,'abund.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_runs = 25\n",
    "plot_every = 0\n",
    "results_folder = './Results'\n",
    "method_name = 'DAEU'\n",
    "for dataset in ['Urban']:\n",
    "    save_folder = results_folder+'/'+method_name+'/'+dataset\n",
    "    if not os.path.exists(save_folder):\n",
    "        os.makedirs(save_folder)\n",
    "    dataset_name = 'synthetic'\n",
    "\n",
    "    hsi = load_HSI(\n",
    "        \"./Datasets/\" + datasetnames[dataset] + \".mat\"\n",
    "    )\n",
    "    data=hsi.array()\n",
    "    params['data']=hsi\n",
    "    params['n_bands']=hsi.bands\n",
    "\n",
    "    for run in range(1,num_runs+1):\n",
    "        training_data = data[np.random.randint(0, data.shape[0], num_spectra), :]\n",
    "        params['opt']=tf.optimizers.RMSprop(learning_rate=learning_rate)\n",
    "        save_name = dataset_name+'_run'+str(run)+'.mat'\n",
    "        save_path = save_folder+'/'+save_name\n",
    "        autoencoder = Autoencoder(params)\n",
    "        autoencoder.fit(training_data,plot_every)\n",
    "        endmembers = autoencoder.get_endmembers()\n",
    "        abundances = autoencoder.get_abundances()\n",
    "        plotEndmembersAndGT(endmembers, hsi.gt)\n",
    "        plotAbundancesSimple(abundances,'abund.png')\n",
    "        sio.savemat(save_path,{'M':endmembers,'A':abundances})\n",
    "        del autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
