{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation of \n",
    "# Spectral-Spatial Hyperspectral Unmixing Using Multitask Learning\n",
    "B. Palsson, J. R. Sveinsson and M. O. Ulfarsson, \"Spectral-Spatial Hyperspectral Unmixing Using Multitask Learning,\" in IEEE Access, vol. 7, pp. 148861-148872, 2019, doi: 10.1109/ACCESS.2019.2944072."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import initializers, constraints, layers, activations, regularizers\n",
    "from tensorflow.python.ops import math_ops\n",
    "from tensorflow.python.keras import backend as K\n",
    "from tensorflow.python.framework import tensor_shape\n",
    "from unmixing import HSI, plotEndmembers,SAD\n",
    "from unmixing import plotEndmembersAndGT, plotAbundancesSimple, load_HSI, PlotWhileTraining\n",
    "from sklearn.feature_extraction.image import extract_patches_2d\n",
    "from scipy import io as sio\n",
    "import os\n",
    "import numpy as np\n",
    "from numpy.linalg import inv\n",
    "import warnings\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "#%warnings.filterwarnings(\"ignore\")\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class SumToOne\n",
    "Also performs regularizations l1 and OSP \n",
    "$$L_\\text{OSP}+\\rho_2\\sum_i L_1(\\bf{h_i})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SumToOne(layers.Layer):\n",
    "    def __init__(self, params, **kwargs):\n",
    "        super(SumToOne, self).__init__(**kwargs)\n",
    "        self.num_outputs = params['num_endmembers']\n",
    "        self.params = params\n",
    "        \n",
    "    def l1_regularization(self,x):\n",
    "        l1 = tf.reduce_sum(tf.abs(x))\n",
    "        return self.params['l1'] * l1\n",
    "    \n",
    "    def osp_regularization(self,x):\n",
    "        return self.params['osp']*OSP(x,self.params['num_endmembers'])\n",
    "        \n",
    "    def call(self, x):\n",
    "        self.add_loss(self.l1_regularization(x))\n",
    "        x = tf.nn.softmax(self.params['scale'] * x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class Autoencoder\n",
    "Wrapper class for the autoencoder model and associcated utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder(object):\n",
    "    def __init__(self, params):\n",
    "        self.data = None\n",
    "        self.params = params\n",
    "        self.model = self.create_model()\n",
    "        self.model.compile(optimizer=self.params[\"optimizer\"], loss=self.params[\"loss\"])\n",
    "    \n",
    "        \n",
    "    def create_model(self):\n",
    "        input_ = []\n",
    "        output = []\n",
    "        #init = initializers.glorot_normal()\n",
    "        init = initializers.RandomNormal(0.01, 0.1)\n",
    "        init2 = initializers.RandomNormal(0.01, 0.1)\n",
    "        for i in range(n_inputs):\n",
    "            input_.append(layers.Input(shape=(self.params['n_bands'],), name='input' + str(i)))\n",
    "        concatenated = layers.concatenate([input_[i] for i in range(n_inputs)])\n",
    "        concatenated = layers.GaussianNoise(0.05)(concatenated)\n",
    "        \n",
    "        dense = layers.Dense(units=(self.params['n_bands'] * self.params['n_inputs']) // 4,\n",
    "                      name='dense1',\n",
    "                      activation=self.params['activation'],\n",
    "                      use_bias=False,\n",
    "                      kernel_initializer=init)(concatenated)\n",
    "        dense = layers.Dropout(0.5)(dense)\n",
    "        dense = layers.BatchNormalization()(dense)\n",
    "        endmembers = layers.Dense(units=self.params['n_bands'],\n",
    "                           activation='linear',\n",
    "                           name='endmembers',\n",
    "                           use_bias=False,\n",
    "                           kernel_constraint=constraints.non_neg(),\n",
    "                           kernel_initializer=init)\n",
    "        for i in range(n_inputs):\n",
    "            abund = layers.Dense(units=self.params['num_endmembers'],\n",
    "                          name='dense_3' + str(i),\n",
    "                          activation=self.params['activation'],\n",
    "                          use_bias=False,\n",
    "                          kernel_initializer=init)(dense)\n",
    "            abund = layers.BatchNormalization()(abund)\n",
    "            abund = layers.Dropout(0.1)(abund)\n",
    "            abund = SumToOne(self.params, name='abundances' + str(i))(abund)\n",
    "            output.append(endmembers(abund))\n",
    "\n",
    "        return tf.keras.Model(inputs=[input_[i] for i in range(n_inputs)], outputs=[output[i] for i in range(n_inputs)])\n",
    "    \n",
    "    def fit(self,data):\n",
    "        self.data = data\n",
    "        num_inputs = self.params['n_inputs']\n",
    "        return self.model.fit(\n",
    "            [self.data[:, i, :] for i in range(num_inputs)],\n",
    "            [self.data[:, i, :] for i in range(num_inputs)],\n",
    "            batch_size=self.params[\"batch_size\"],\n",
    "            epochs=self.params[\"epochs\"],\n",
    "            verbose=1\n",
    "        )\n",
    "    \n",
    "\n",
    "    def get_endmembers(self):\n",
    "        return self.model.layers[len(self.model.layers) - 1].get_weights()[0]\n",
    "\n",
    "    def get_abundances(self):\n",
    "        intermediate_layer_model = tf.keras.Model(inputs=self.model.input,\n",
    "                                         outputs=[self.model.get_layer('abundances' + str(i)).output for i in\n",
    "                                                  range(self.params['n_inputs'])])\n",
    "        abundances = np.mean(intermediate_layer_model.predict([self.params['data'].array() for i in range(self.params['n_inputs'])]),\n",
    "                             axis=0)\n",
    "        abundances = np.reshape(abundances,[self.params['data'].cols,self.params['data'].rows,self.params['num_endmembers']])\n",
    "        return abundances\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method make_patches\n",
    "Makes $n\\times n$ patches for MTL unmixer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_patches(hsi, n, num_patches):\n",
    "    patch_size = n\n",
    "    data = extract_patches_2d(hsi.image, (n, n), num_patches)\n",
    "    s = data.shape\n",
    "    data = data.reshape(s[0], n * n, hsi.bands)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data and set Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dictonary of aliases for datasets. The first string is the key and second is value (name of matfile without .mat suffix)\n",
    "#Useful when looping over datasets\n",
    "datasetnames = {\n",
    "        \"Urban\": \"Urban4\",\n",
    "}\n",
    "\n",
    "dataset = \"Urban\"\n",
    "\n",
    "hsi = load_HSI(\n",
    "    \"./Datasets/\" +datasetnames[dataset] + \".mat\"\n",
    ")\n",
    "\n",
    "# Hyperparameters\n",
    "num_endmembers = 4\n",
    "n_inputs = 4\n",
    "num_patches = 2000\n",
    "batch_size = 15\n",
    "learning_rate = 0.001\n",
    "epochs = 60\n",
    "loss = SAD\n",
    "activation = layers.LeakyReLU(0.2)\n",
    "l1 = 0\n",
    "\n",
    "# Hyperparameter dictionary\n",
    "params = {\n",
    "    'n_inputs':n_inputs,\n",
    "    \"activation\": activation,\n",
    "    \"num_endmembers\": num_endmembers,\n",
    "    \"batch_size\": batch_size,\n",
    "    \"num_patches\": num_patches,\n",
    "    \"data\": hsi,\n",
    "    \"epochs\": epochs,\n",
    "    \"n_bands\": hsi.bands,\n",
    "    \"GT\": hsi.gt,\n",
    "    \"lr\": learning_rate,\n",
    "    \"optimizer\": tf.optimizers.RMSprop(learning_rate=learning_rate,decay=0.000),\n",
    "    \"loss\": loss,\n",
    "    \"scale\": 3,\n",
    "    \"l1\": l1,\n",
    "}\n",
    "\n",
    "training_data = make_patches(hsi,params['n_inputs'],params['num_patches'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run experiment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_runs = 25\n",
    "results_folder = './Results'\n",
    "method_name = 'MTAEU'\n",
    "\n",
    "for dataset in ['Urban']:\n",
    "    save_folder = results_folder+'/'+method_name+'/'+dataset\n",
    "    if not os.path.exists(save_folder):\n",
    "        os.makedirs(save_folder)\n",
    "    hsi = load_HSI(\"./Datasets/\" + datasetnames[dataset] + \".mat\")\n",
    "\n",
    "    for run in range(1,num_runs+1):\n",
    "        print('run nr: '+str(run)+'\\n')\n",
    "        params = {\n",
    "        'n_inputs':n_inputs,\n",
    "        \"activation\": activation,\n",
    "        \"num_endmembers\": num_endmembers,\n",
    "        \"batch_size\": batch_size,\n",
    "        \"num_spectra\": num_spectra,\n",
    "        \"data\": hsi,\n",
    "        \"epochs\": epochs,\n",
    "        \"n_bands\": hsi.bands,\n",
    "        \"GT\": hsi.gt,\n",
    "        \"lr\": learning_rate,\n",
    "        \"optimizer\": tf.optimizers.RMSprop(learning_rate=learning_rate,decay=0.0001),\n",
    "        \"loss\": loss,\n",
    "        \"scale\": 3,\n",
    "        \"l1\": l1,\n",
    "        }\n",
    "        training_data = make_patches(hsi,params['n_inputs'],params['num_spectra'])\n",
    "        save_folder = results_folder+'/'+method_name+'/'+dataset\n",
    "        save_name = dataset+'_run'+str(run)+'.mat'\n",
    "        save_path = save_folder+'/'+save_name\n",
    "        autoencoder = Autoencoder(params)\n",
    "        autoencoder.fit(training_data)\n",
    "        endmembers = autoencoder.get_endmembers()\n",
    "        abundances = autoencoder.get_abundances()\n",
    "        plotEndmembersAndGT(endmembers, hsi.gt)\n",
    "        plotAbundancesSimple(abundances,'abund.png')\n",
    "        sio.savemat(save_path,{'M':endmembers,'A':abundances})\n",
    "        del autoencoder\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
